{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de imágenes usando una red profunda\n",
    "# Dataset de perros y gatos\n",
    "\n",
    "**Profesor:** Roberto Muñoz <br />\n",
    "**E-mail:** <rmunoz@metricarts.com> <br />\n",
    "\n",
    "**Colaborador:** Sebastián Arpón <br />\n",
    "**E-mail:** <rmunoz@metricarts.com> <br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import os\n",
    "# os.environ('TF_CCP_MIN_LOG_LEVEL')='3' # ni idea que es esto\n",
    "\n",
    "import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_dir(directory):\n",
    "    cats = glob.glob(os.path.join(directory,\"cats\") + '/*.jpg')\n",
    "    dogs = glob.glob(os.path.join(directory,\"dogs\") + '/*.jpg')\n",
    "    m_images = cats + dogs\n",
    "    m_labels = []\n",
    "    m_labels.extend([CAT] * len(cats))\n",
    "    m_labels.extend([DOG] * len(dogs))\n",
    "    assert len(m_labels) == len(m_images)\n",
    "    LABELS_DIMENSIONS = 2\n",
    "    m_labels = tf.one_hot(m_labels, LABELS_DIMENSIONS)\n",
    "    print(\"Encontre %d imagenes y etiquetas en %s\" %(len(m_images),directory))\n",
    "    return m_images, m_labels\n",
    "\n",
    "def load_image(path_to_image, p_label):\n",
    "    m_label = p_label\n",
    "    m_image = tf.read_file(path_to_image)\n",
    "    m_image = tf.image.decode_jpeg(m_image)\n",
    "    m_image = tf.image.resize_images(m_image,(150,150))\n",
    "    m_image = m_image / 255\n",
    "    return m_image, m_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/cats_and_dogs_small\"\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "val_dir = os.path.join(data_dir , \"validation\")\n",
    "\n",
    "CAT = 0\n",
    "DOG = 1\n",
    "\n",
    "print(\"Carpeta con imagenes para el entrenamiento: \", train_dir)\n",
    "print(\"Carpeta con imagenes para la evaluación: \", test_dir)\n",
    "\n",
    "train_images, train_labels = read_dir(train_dir)\n",
    "test_images, test_labels = read_dir(test_dir)\n",
    "val_images, val_labels = read_dir(val_dir)\n",
    "print (\"=============================================================\")\n",
    "print (\"=============================================================\")\n",
    "print (test_images)\n",
    "print (\"=============================================================\")\n",
    "print (\"=============================================================\")\n",
    "print (test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=100\n",
    "\n",
    "img, label = load_image(train_images[i],\"\")\n",
    "plt.imshow(img)\n",
    "print(train_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "buffer_size = 300\n",
    "\n",
    "train_data_set = tf.data.Dataset.from_tensor_slices((train_images,train_labels)).shuffle(buffer_size).map(load_image).batch(batch_size)\n",
    "val_data_set = tf.data.Dataset.from_tensor_slices((val_images,val_labels)).shuffle(buffer_size).map(load_image).batch(20)                                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definimos la arquitectura de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(tf.layers.Conv2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.layers.Conv2D(32,(3,3),activation=tf.nn.relu, input_shape=(150,150,3)))\n",
    "model.add(tf.layers.MaxPooling2D(pool_size=(2,2),strides=2))\n",
    "model.add(tf.layers.Conv2D(64,(3,3),activation=tf.nn.relu))\n",
    "model.add(tf.layers.MaxPooling2D(pool_size=(2,2),strides=2))\n",
    "model.add(tf.layers.Conv2D(128,(3,3),activation=tf.nn.relu))\n",
    "model.add(tf.layers.MaxPooling2D(pool_size=(2,2),strides=2))\n",
    "model.add(tf.layers.Conv2D(128,(3,3),activation=tf.nn.relu))\n",
    "model.add(tf.layers.MaxPooling2D(pool_size=(2,2),strides=2))\n",
    "model.add(tf.layers.Flatten())\n",
    "model.add(tf.layers.Dense(512,activation=tf.nn.relu))\n",
    "model.add(tf.layers.Dense(2,activation=tf.nn.softmax))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usamos el AdamOptimizer como funcion de optimización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "batch_loss_list=[]\n",
    "val_loss_list=[]\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for (batch,(images, labels)) in enumerate(train_data_set):\n",
    "        batch_loss, batch_accuracy = model.train_on_batch(images.numpy(), labels.numpy())\n",
    "        batch_loss_list.append(float(batch_loss))\n",
    "        for (dummy,(val_im, val_lab)) in enumerate(val_data_set):\n",
    "            val_loss, val_accuracy = model.evaluate(val_im.numpy(),val_lab.numpy())\n",
    "            val_loss_list.append(float(val_loss))\n",
    "        if batch%5 == 0:\n",
    "            print('Entrenamiento Epoca #%d\\t Loss: %.6f\\t Accuracy:  %.6f\\t'% (epoch+1, batch_loss, batch_accuracy))\n",
    "            print('Validacion Epoca #%d\\t Loss: %.6f\\t Accuracy:  %.6f\\t' % (epoch+1, val_loss, val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficamos la evolución de la función de costo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(8,6))\n",
    "\n",
    "ax.plot(batch_loss_list, label=\"train\")\n",
    "ax.plot(val_loss_list, label=\"test\")\n",
    "\n",
    "plt.xlabel(\"iteración\")\n",
    "plt.ylabel(\"Función de costo - train\")\n",
    "plt.legend()\n",
    "\n",
    "plt.title(\"Tain dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

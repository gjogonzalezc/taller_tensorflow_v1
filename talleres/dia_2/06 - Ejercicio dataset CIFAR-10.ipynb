{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio dataset CIFAR-10\n",
    "\n",
    "**Profesor:** Roberto Muñoz <br />\n",
    "**E-mail:** <rmunoz@metricarts.com> <br />\n",
    "\n",
    "**Colaborador:** Sebastián Arpón <br />\n",
    "**E-mail:** <rmunoz@metricarts.com> <br />\n",
    "\n",
    "En este taller usaremos el dataset CIFAR-10 (https://www.cs.toronto.edu/~kriz/cifar.html), el cual consiste de 60000 imagenes a color de tamaño 32x32 pixels y clasificado en 10 clases. Cada clase consiste de 6000 imágenes.\n",
    "\n",
    "Tutoriales y documentación de Tensorflow\n",
    "\n",
    "- https://www.tensorflow.org/get_started/get_started\n",
    "- https://www.tensorflow.org/api_docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar datasets de CIFAR-10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tools.data_utils import load_CIFAR10\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'data/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de ejemplo\n",
    "\n",
    "### Algunas datos utiles\n",
    "\n",
    ". Nuestros dataset de imágenes es inicialmente N x H x W x C, donde\n",
    "* N es numero de registros\n",
    "* H es la altura de cada imagen en pixels\n",
    "* W es el ancho de cada imagen en pixels\n",
    "* C es el numero de canales (comunmente 3: R, G, B)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El modelo de ejemplo en sí\n",
    "\n",
    "El primer paso es definir la arquitectura de nuestro modelo\n",
    "\n",
    "Aquí hay un ejemplo de una red neuronal convolucional definida en TensorFlow -- intente entender que está haciendo cada línea, recordando que cada capa está construida sobre la capa anterior. No hemos entrenado nada aún - eso vendrá después - por ahora, queremos que entienda como se configura todo.\n",
    "\n",
    "En este ejemplo, se ven capas convolucionales 2D (Conv2d), activaciones RELU, y capas completamente conectadas (Lineales). También se ven siendo usadas la función de pérdida de Hinge y el optimizador de Adam.\n",
    "\n",
    "Asegúrese de que entiende por qué los parámetros de la capa lineal son 5408 y 10.\n",
    "\n",
    "### Detalles TensorFlow\n",
    "En TensorFlow, así como en nuestros notebooks anteriores, primero inicializaremos nuestras variables y luego nuestro modelo de red.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clear old variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# setup input (e.g. the data that changes every batch)\n",
    "# The first dim is None, and gets sets automatically based on batch size fed in\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "def simple_model(X,y):\n",
    "    # define our weights (e.g. init_two_layer_convnet)\n",
    "    \n",
    "    # setup variables\n",
    "    Wconv1 = tf.get_variable(\"Wconv1\", shape=[7, 7, 3, 32])\n",
    "    bconv1 = tf.get_variable(\"bconv1\", shape=[32])\n",
    "    W1 = tf.get_variable(\"W1\", shape=[5408, 10])\n",
    "    b1 = tf.get_variable(\"b1\", shape=[10])\n",
    "\n",
    "    # define our graph (e.g. two_layer_convnet)\n",
    "    a1 = tf.nn.conv2d(X, Wconv1, strides=[1,2,2,1], padding='VALID') + bconv1\n",
    "    h1 = tf.nn.relu(a1)\n",
    "    h1_flat = tf.reshape(h1,[-1,5408])\n",
    "    y_out = tf.matmul(h1_flat,W1) + b1\n",
    "    return y_out\n",
    "\n",
    "y_out = simple_model(X,y)\n",
    "\n",
    "# define our loss\n",
    "total_loss = tf.losses.hinge_loss(tf.one_hot(y,10),logits=y_out)\n",
    "mean_loss = tf.reduce_mean(total_loss)\n",
    "\n",
    "# define our optimizer\n",
    "optimizer = tf.train.AdamOptimizer(5e-4) # select optimizer and set learning rate\n",
    "train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow soporta muchos otros tipos de capas, funciones de pérdida y optimizadores - experimentará con estos a continuación. Aquí está la documentación oficial de la API para estos (si alguno de los parámetros usados arriba no fueron claros, este recurso también será útil).\n",
    "\n",
    "* Capas, Activaciones, Funciones de Pérdida : https://www.tensorflow.org/api_guides/python/nn\n",
    "* Optimizadores: https://www.tensorflow.org/api_guides/python/train#Optimizers\n",
    "* BatchNorm: https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenando el Módelo en una Época\n",
    "Mientras hemos definido un \"graph\" de operaciones arriba entregándole datos de entrada y computando los resultados, para ejecutar \"Graphs\" de TensorFlow primero debemos crear un objeto `tf.Session`. Una `tf.Session` encapsula el control y el estado de tiempo de ejecución de TensorFlow. Para más información, vea la guía de TensorFlow [Comenzando con TensorFlow](https://www.tensorflow.org/get_started/get_started).\n",
    "\n",
    "Opcionalmente también podemos especificar el contexto de dispositivo, como por ejemplo `/cpu:0` o `/gpu:0`. Para documentación sobre este comportamiento vea [esta guía de TensorFlow](https://www.tensorflow.org/tutorials/using_gpu)\n",
    "\n",
    "Abajo debería ver una pérdida de validación entre 0.4 y 0.6, y una precisión entre 0.30 y 0.35."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(session, predict, loss_val, Xd, yd,\n",
    "              epochs=1, batch_size=64, print_every=100,\n",
    "              training=None, plot_losses=False):\n",
    "    # have tensorflow compute accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(predict,1), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # shuffle indicies\n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    np.random.shuffle(train_indicies)\n",
    "\n",
    "    training_now = training is not None\n",
    "    \n",
    "    # setting up variables we want to compute (and optimizing)\n",
    "    # if we have a training function, add that to things we compute\n",
    "    variables = [mean_loss,correct_prediction,accuracy]\n",
    "    if training_now:\n",
    "        variables[-1] = training\n",
    "    \n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    for e in range(epochs):\n",
    "        # keep track of losses and accuracy\n",
    "        correct = 0\n",
    "        losses = []\n",
    "        # make sure we iterate over the dataset once\n",
    "        for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%Xd.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: Xd[idx,:],\n",
    "                         y: yd[idx],\n",
    "                         is_training: training_now }\n",
    "            # get batch size\n",
    "            actual_batch_size = yd[idx].shape[0]\n",
    "            \n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "            loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n",
    "            \n",
    "            # aggregate performance stats\n",
    "            losses.append(loss*actual_batch_size)\n",
    "            correct += np.sum(corr)\n",
    "            \n",
    "            # print every now and then\n",
    "            if training_now and (iter_cnt % print_every) == 0:\n",
    "                print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                      .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "            iter_cnt += 1\n",
    "        total_correct = correct/Xd.shape[0]\n",
    "        total_loss = np.sum(losses)/Xd.shape[0]\n",
    "        print(\"Epoch {2}, Overall loss = {0:.3g} and accuracy of {1:.3g}\"\\\n",
    "              .format(total_loss,total_correct,e+1))\n",
    "        if plot_losses:\n",
    "            plt.plot(losses)\n",
    "            plt.grid(True)\n",
    "            plt.title('Epoch {} Loss'.format(e+1))\n",
    "            plt.xlabel('minibatch number')\n",
    "            plt.ylabel('minibatch loss')\n",
    "            plt.show()\n",
    "    return total_loss,total_correct\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    with tf.device(\"/cpu:0\"): #\"/cpu:0\" or \"/gpu:0\" \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print('Training')\n",
    "        run_model(sess,y_out,mean_loss,X_train,y_train,1,64,100,train_step,True)\n",
    "        print('Validation')\n",
    "        run_model(sess,y_out,mean_loss,X_val,y_val,1,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenando un modelo específico\n",
    "En esta sección vamos a especificar un modelo para que usted construya. El objetivo no es obtener un buen rendimiento (eso será después), si no que en vez acostumbrarse y familiarizarse con entender la documentación de TensorFlow y la configuración de su propio modelo. \n",
    "\n",
    "Usando el código provisto arriba como guía, y usando la documentación de TensorFlow apropiada, especifique un modelo con la siguiente arquitectura:\n",
    "\n",
    "* 7x7 Convolutional Layer with 32 filters and stride of 1\n",
    "* ReLU Activation Layer\n",
    "* Spatial Batch Normalization Layer (trainable parameters, with scale and centering)\n",
    "* 2x2 Max Pooling layer with a stride of 2\n",
    "* Affine layer with 1024 output units\n",
    "* ReLU Activation Layer\n",
    "* Affine layer from 1024 input units to 10 outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clear old variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# define our input (e.g. the data that changes every batch)\n",
    "# The first dim is None, and gets sets automatically based on batch size fed in\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "# define model\n",
    "def complex_model(X,y,is_training):\n",
    "    \n",
    "    # Set up variables\n",
    "    Wconv1 = tf.get_variable(\"Wconv1\",shape=[7,7,3,32])\n",
    "    bconv1 = tf.get_variable(\"bconv1\",shape=[32])\n",
    "    Waffine1 = tf.get_variable(\"Waffine1\", shape=[5408,1024])\n",
    "    baffine1 = tf.get_variable(\"baffine1\", shape=[1024])\n",
    "    Waffine2 = tf.get_variable(\"Waffine2\", shape=[1024,10])\n",
    "    baffine2 = tf.get_variable(\"baffine2\", shape=[10])\n",
    "    \n",
    "    # Define model\n",
    "    conv1act = tf.nn.conv2d(X, Wconv1, strides=[1,1,1,1], padding='VALID') + bconv1\n",
    "    relu1act = tf.nn.relu(conv1act)\n",
    "    batchnorm1act = tf.layers.batch_normalization(relu1act, training=is_training)  # Using the tf.layers batch norm as its easier\n",
    "    pool1act = tf.nn.max_pool(batchnorm1act, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    flatten1 = tf.reshape(pool1act,[-1,5408])\n",
    "    affine1act = tf.matmul(flatten1, Waffine1) + baffine1\n",
    "    relu2act = tf.nn.relu(affine1act)\n",
    "    y_out = tf.matmul(relu2act, Waffine2) + baffine2\n",
    "    \n",
    "    return y_out\n",
    "\n",
    "y_out = complex_model(X,y,is_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para asegurarse de que está haciendo el ejercicio de manera correcta, use la siguiente herramienta para revisar la dimensionalidad de su salida (debería ser 64 x 10, debido a que nuestros \"batches\" tiene un tamaño de 64 y el output de la \"affine layer\" final debería ser 10, correspondiendo a nuestras 10 clases):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we're going to feed a random batch into the model \n",
    "# and make sure the output is the right size\n",
    "x = np.random.randn(64, 32, 32,3)\n",
    "with tf.Session() as sess:\n",
    "    with tf.device(\"/cpu:0\"): #\"/cpu:0\" or \"/gpu:0\"\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        ans = sess.run(y_out,feed_dict={X:x,is_training:True})\n",
    "        %timeit sess.run(y_out,feed_dict={X:x,is_training:True})\n",
    "        print(ans.shape)\n",
    "        print(np.array_equal(ans.shape, np.array([64, 10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debería ver lo siguiente después de ejecutar el código de arriba:\n",
    "\n",
    "`(64, 10)`\n",
    "\n",
    "`True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU!\n",
    "Ahora, vamos a intentar ejecutar el modelo en un dispositivo GPU, el resto del código se queda sin cambios y todas nuestras variables y operaciones serán computadas usando rutas de código acelerada. Sin embargo, si no hay GPU, obtendremos una excepción de python y tendremos que reconstruir nuestro \"graph\". En una CPU de dos núcleos, ejecutando el código de arriba podría ver alrededor de 50-80ms/batch, mientras que en las GPUs de Google Cloud (ejecute el código de abajo) debería ser alrededor de 2-5ms/batch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    with tf.Session() as sess:\n",
    "        with tf.device(\"/gpu:0\") as dev: #\"/cpu:0\" or \"/gpu:0\"\n",
    "            tf.global_variables_initializer().run()\n",
    "\n",
    "            ans = sess.run(y_out,feed_dict={X:x,is_training:True})\n",
    "            %timeit sess.run(y_out,feed_dict={X:x,is_training:True})\n",
    "except tf.errors.InvalidArgumentError:\n",
    "    print(\"no gpu found, please use Google Cloud if you want GPU acceleration\")    \n",
    "    # rebuild the graph\n",
    "    # trying to start a GPU throws an exception \n",
    "    # and also trashes the original graph\n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "    y = tf.placeholder(tf.int64, [None])\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    y_out = complex_model(X,y,is_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debería observar que incluso un simple \"forward pass\" como este es ejecutado significativamente más rápido en GPU. Por lo tanto, para el resto de la tarea (y cuando vaya a entrenar sus modelos en la tarea 3 y su proyecto), debería usar dispositivos GPU. Sin embargo, con TensorFlow, el dispositivo por defecto es una GPU si está disponible, y una CPU en otro caso, por lo que podemos omitir la especificación del dispositivo desde ahora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenando el Modelo.\n",
    "Ahora que ha visto como definir un modelo y hacer un \"forward pass\" de datos por él, veamos cómo entrenaría una época completa sobre sus datos de entrenamiento (usando el \"complex_model\" creado más arriba).\n",
    "\n",
    "Asegúrese que entiende como cada función de TensorFlow usada abajo corresponde a lo que usted implementó en su red neuronal personalizada.\n",
    "\n",
    "Primero, configure un **RMSprop optimizer** (usando una taza de aprendizaje de 1e-3) y una función **cross-entropy loss**. Vea la documentación de TensorFlow para más información:\n",
    "\n",
    "* Capas, Activaciones, Funciones de Pérdida : https://www.tensorflow.org/api_guides/python/nn\n",
    "* Optimizadores: https://www.tensorflow.org/api_guides/python/train#Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inputs\n",
    "#     y_out: is what your model computes\n",
    "#     y: is your TensorFlow variable with label information\n",
    "# Outputs\n",
    "#    mean_loss: a TensorFlow variable (scalar) with numerical loss\n",
    "#    optimizer: a TensorFlow optimizer\n",
    "# This should be ~3 lines of code!\n",
    "mean_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(y,10), logits=y_out))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batch normalization in tensorflow requires this extra dependency\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrene el Modelo\n",
    "Abajo creáremos una `tf.session` y entrenaremos el modelo en una época. Debería ver una pérdida de 1.4 a 2.0 y una precisión de 0.4 a 0.5. Habrá alguna variación debido a las semillas de los números aleatorios y diferencias en inicialización.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('Training')\n",
    "run_model(sess,y_out,mean_loss,X_train,y_train,1,64,100,train_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisando la Precisión del Modelo.\n",
    "Veamos el código de entrenamiento y el código de prueba en acción -- siéntase libre de usar estos métodos cuando evalué los modelos que desarrolle abajo. Debería ver una pérdida de 1.3 a 2.0 con una precisión de 0.45 a 0.55.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Validation')\n",
    "run_model(sess,y_out,mean_loss,X_val,y_val,1,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar un _gran_ modelo en CIFAR-10!\n",
    "\n",
    "Ahora es su trabajo experimentar con arquitecturas, hiperparámetros, funciones de pérdida, y optimizadores para entrenar un modelo que logre ** >= 70% de presición en su conjunto de validación** de CIFAR-10. Puede usar la función `run_model` de arriba.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosas que debería intentar:\n",
    "- **Tamaño de Filtro**: Arriba usamos 7x7; Esto hace bonitas figuras, pero filtros más pequeños podrían ser más eficientes.\n",
    "- **Número de Filtros**: Arriba usamos 32 filtros. ¿Más o menos funcionan mejor?\n",
    "- **Convolución: Pooling vs Desnuda**: Usamos max pooling o o simplemente convoluciones desnudas?\n",
    "- **Batch normalization**: Intente añadir \"batch normalization\" espacial después de las capas de convolución y \"batch normalization\" vainilla después de \"affine layers\". ¿Entrenan más rápido sus redes?\n",
    "- **Arquitectura de la Red**: La red de arriba tiene dos capas de parámetros entrenables. ¿Se puede mejorar usando una red profunda? Buenas arquitecturas intentan incluir:\n",
    "    - [conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]\n",
    "    - [conv-relu-conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]\n",
    "    - [batchnorm-relu-conv]xN -> [affine]xM -> [softmax or SVM]\n",
    "- **Use \"TensorFlow Scope\"**: Use \"TensorFlow scope\" y/o [tf.layers](https://www.tensorflow.org/api_docs/python/tf/layers) para facilitar la escritura de redes profundas. Vea [this tutorial](https://www.tensorflow.org/tutorials/layers) para cómo usar `tf.layers`.\n",
    "- **Use Decaimiento de Taza de Aprendizaje**: [Como indican las notas](http://cs231n.github.io/neural-networks-3/#anneal), decaimiento en la taza de aprendizaje puede ayudarle al modelo a converger. Cuando la pérdida no cambie en una época completa, u otra heurística que encuentre apropiada, siéntase libre de disminuir la taza. Vea la [Documentación de Tensorflow](https://www.tensorflow.org/versions/master/api_guides/python/train#Decaying_the_learning_rate) para decaimiento de taza de aprendizaje.\n",
    "- **Global Average Pooling**: En lugar de aplanar y tener múltiples \"affine layers\", realice convoluciones hasta que la imagen se ponga pequeña (7x7 o más) y luego realice una operación de \"average pooling\" para obtener una imagen de 1x1 (1, 1, Filtro#), que luego se reconfigurará a un (Filtro#) vector. Esto se utiliza en [Google's Inception Network](https://arxiv.org/abs/1512.00567) (Consulte la tabla 1 para ver su arquitectura).\n",
    "- **Regularización**: Agregue una regulirización de pesos L2, o quizás use [Dropout como en el tutorial TensorFlow de MNIST](https://www.tensorflow.org/get_started/mnist/pros)\n",
    "\n",
    "### Consejos para entrenamiento\n",
    "Para cada arquitectura de red que pruebe, debería ajustar la taza de entrenamiento y la intensidad de regularización. Cuando haga esto hay un par de cosas importantes que debe tener en mente:\n",
    "\n",
    "- Si los parámetros están funcionando bien, debería ver mejora dentro de un par de cientos de iteraciones\n",
    "- Recuerde una aproximación de grueso a fino para la afinación de hiperparámetros: comience por probar un largo rango de hiperparámetros durante pocas iteraciones de entrenamiento para encontrar la combinaciones de parámetros que están funcionando.\n",
    "- Una vez que encuentre un conjunto de parámetros que parezca funcionar, busque más cuidadosamente cerca de estos parámetros. Puede que necesite entrenar durante más épocas.\n",
    "- Debería usar el conjunto de validación para la búsqueda de hiperparámetros, y guardaremos el conjunto de prueba para evaluar su arquitectura construida en los mejores parámetros seleccionados por el conjunto de validación.\n",
    "\n",
    "\n",
    "### Ir mucho más allá\n",
    "Si se siente aventurero, puede implementar muchas otras funciones para intentar mejorar tu rendimiento. **No se requiere** implementar ninguno de estos; sin embargo, serían buenas cosas para intentar obtener crédito adicional.\n",
    "\n",
    "- Pasos de actualización alternativos: para la tarea implementamos SGD+momentum, RMSprop y Adam; podría probar alternativas como AdaGrad o AdaDelta.\n",
    "- Funciones de activación alternativas como leaky ReLU, ReLU paramétrica, ELU o MaxOut.\n",
    "- Model ensembles\n",
    "- Data augmentation\n",
    "- Nuevas Arquitecturas\n",
    "  - [ResNets](https://arxiv.org/abs/1512.03385) donde la entrada de la capa anterior es añadida a la salida.\n",
    "  - [DenseNets](https://arxiv.org/abs/1608.06993) donde las entradas a capas previas se concatenan juntas.\n",
    "  - [This blog has an in-depth overview](https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32)\n",
    "\n",
    "Si decide implementar algo extra, describa claramente en la celda \"Extra Credit Description\" abajo.\n",
    "\n",
    "### Lo que esperamos\n",
    "Como mínimo, debería ser capaz de entrenar una ConvNet que obtenga **> = 70% de precisión en el conjunto de validación**. Este es solo un límite inferior: si tiene cuidado, ¡debería ser posible obtener precisiones mucho más altas que esa! Se otorgarán créditos adicionales para modelos particularmente buenos o con enfoques únicos.\n",
    "\n",
    "Debe usar el espacio a continuación para experimentar y entrenar su red. La última celda de este notebook debe contener las precisiones del conjunto de entrenamiento y validación para su red entrenada final.\n",
    "\n",
    "Diviértete y feliz entrenamiento!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feel free to play with this cell\n",
    "\n",
    "def my_model(X,y,is_training):\n",
    "    \n",
    "    # Conv-Relu-BN\n",
    "    conv1act = tf.layers.conv2d(inputs=X, filters=32, padding='same', kernel_size=3, strides=1, activation=tf.nn.relu)\n",
    "    bn1act = tf.layers.batch_normalization(inputs=conv1act, training=is_training)\n",
    "    # Conv-Relu-BN\n",
    "    conv2act = tf.layers.conv2d(inputs=bn1act, filters=64, padding='same', kernel_size=3, strides=1, activation=tf.nn.relu)\n",
    "    bn2act = tf.layers.batch_normalization(inputs=conv2act, training=is_training)\n",
    "    # Maxpool\n",
    "    maxpool1act = tf.layers.max_pooling2d(inputs=bn2act, pool_size=2, strides=2)\n",
    "    # Flatten\n",
    "    flatten1 = tf.reshape(maxpool1act,[-1,16384])\n",
    "    # FC-Relu-BN\n",
    "    fc1 = tf.layers.dense(inputs=flatten1, units=1024 , activation=tf.nn.relu)\n",
    "    bn3act = tf.layers.batch_normalization(inputs=fc1, training=is_training)\n",
    "    # Output FC \n",
    "    y_out = tf.layers.dense(inputs=bn3act, units=10, activation=None)\n",
    "    \n",
    "    return y_out\n",
    "    \n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "y_out = my_model(X,y,is_training)\n",
    "\n",
    "mean_loss = tf.losses.softmax_cross_entropy(logits=y_out, onehot_labels=tf.one_hot(y,10))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "\n",
    "# batch normalization in tensorflow requires this extra dependency\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feel free to play with this cell\n",
    "# This default code creates a session\n",
    "# and trains your model for 10 epochs\n",
    "# then prints the validation set accuracy\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('Training')\n",
    "run_model(sess,y_out,mean_loss,X_train,y_train,10,64,100,train_step,True)\n",
    "print('Validation')\n",
    "run_model(sess,y_out,mean_loss,X_val,y_val,1,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test your model here, and make sure \n",
    "# the output of this cell is the accuracy\n",
    "# of your best model on the training and val sets\n",
    "# We're looking for >= 70% accuracy on Validation\n",
    "print('Training')\n",
    "run_model(sess,y_out,mean_loss,X_train,y_train,1,64)\n",
    "print('Validation')\n",
    "run_model(sess,y_out,mean_loss,X_val,y_val,1,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describa lo que hizo aquí\n",
    "En esta celda debería escribir una explicación de lo que hizo, cualquier características adicionales que haya implementado, y cualquier visualización o \"graphs\" que haga en el proceso de entrnamiento y evaluación de red\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added an extra conv layer, changed all conv layers to 3x3 kernels. Added batch norm for the first fully connected layer. Change to Adam optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set - Do this only once\n",
    "Now that we've gotten a result that we're happy with, we test our final model on the test set. This would be the score we would achieve on a competition. Think about how this compares to your validation set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Test')\n",
    "run_model(sess,y_out,mean_loss,X_test,y_test,1,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going further with TensorFlow\n",
    "\n",
    "The next assignment will make heavy use of TensorFlow. You might also find it useful for your projects. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit Description\n",
    "If you implement any additional features for extra credit, clearly describe them here with pointers to any code in this or other files if applicable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
